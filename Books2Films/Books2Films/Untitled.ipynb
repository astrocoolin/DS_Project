{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Book:\n",
    "    def __init__(self, name, nofeatures):\n",
    "        self.name = name\n",
    "        self.webpage = \"\"\n",
    "        self.title = \"\"\n",
    "        self.coverlink = \"\"\n",
    "        self.keywords = []\n",
    "        self.nofeatures = nofeatures\n",
    "\n",
    "        self.get_webpage()\n",
    "        self.get_keywords()\n",
    "        self.unikeywords.extend(self.bikeywords)\n",
    "        self.df = pd.DataFrame({'Name': [self.name], 'Keywords': [self.unikeywords]})\n",
    "\n",
    "    def get_webpage(self):\n",
    "        query = self.name + ' site:Goodreads.com'\n",
    "        g_clean = []  # this is the list we store the search results\n",
    "        url = 'https://www.google.com/search?client=ubuntu&channel=fs&q={}&ie=utf-8&oe=utf-8'.format(\n",
    "            query)  # this is the actual query we are going to scrape\n",
    "\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.text)\n",
    "        a = soup.find_all('a')  # a is a list\n",
    "        a[0].get('href')\n",
    "        for i in a:\n",
    "            k = i.get('href')\n",
    "            try:\n",
    "                m = re.search(\"(?P<url>https?://[^\\s]+)\", k)\n",
    "                n = m.group(0)\n",
    "                rul = n.split('&')[0]\n",
    "                domain = urlparse(rul)\n",
    "                if (re.search('google.com', domain.netloc)):\n",
    "                    continue\n",
    "                else:\n",
    "                    g_clean.append(rul)\n",
    "            except:\n",
    "                continue\n",
    "        self.webpage = g_clean[0]\n",
    "\n",
    "    def get_keywords(self):\n",
    "        # get stopwords\n",
    "        stop_nltk = set(stopwords.words(\"english\"))\n",
    "        stop_custom = {'and', 'best', 'book', 'books', 'character', 'characters', 'did', 'end', 'ending', 'film',\n",
    "                       'films', 'great', 'just', 'like', 'make', 'movie', 'movies', 'novel', 'permalink', 'read',\n",
    "                       'review', 'story', 'story', 'the', 'when', 'see', 'seen', 'get', 'many', 'one', 'made',\n",
    "                       'ever', 'every', 'vote', 'much', 'well', 'watch', 'even', 'everything', 'youll', 'would',\n",
    "                       'makes', 'even', 'ive', 'really', 'say', 'two', 'three', 'really', 'time', 'reading', 'read',\n",
    "                       'first', 'going', 'good', 'little', 'new', 'things', 'thing', 'yet', 'us', 'want', 'fiction',\n",
    "                       'science', 'novella', 'people', 'something', 'know', 'though', 'go', 'post', 'back', 'series'\n",
    "                       'year','years', 'http', 'www'}\n",
    "        stop_words = stop_nltk.union(stop_custom)\n",
    "\n",
    "        # get book keywords and title of book, scrape from internet\n",
    "        r = requests.get(self.webpage)\n",
    "        html_soup = BeautifulSoup(r.content)\n",
    "        self.coverlink = html_soup.find('img', id='coverImage')['src']\n",
    "        self.title = html_soup.find('h1', id='bookTitle').text.strip()\n",
    "\n",
    "        # Extract reviews and vectorize the text, returning the keywords\n",
    "        review_containers = html_soup.find_all('div', class_='review')\n",
    "        for i, review in enumerate(review_containers):\n",
    "            review_containers[i] = review.find('span', class_='readable').text.encode(\"ascii\",\"ignore\").strip()\n",
    "        if len(review_containers) < 2:\n",
    "            return\n",
    "        \n",
    "    def vectorize(self, keyword, degree)\n",
    "        Vect = TfidfVectorizer(max_features=self.nofeatures, ngram_range=(degree,degree), stop_words=stop_words, max_df=0.80)\n",
    "        Book_Vect = Vect.fit_transform(review_containers)\n",
    "        Book_feature_names = list(Vect.get_feature_names())\n",
    "        keyword = Book_feature_names\n",
    "        \n",
    "        biVect = TfidfVectorizer(max_features=self.nofeatures, ngram_range=(2,2), stop_words=stop_words, max_df=0.80)\n",
    "        biBook_Vect = biVect.fit_transform(review_containers)\n",
    "        biBook_feature_names = list(biVect.get_feature_names())\n",
    "        self.unikeywords = uniBook_feature_names\n",
    "        self.bikeywords = biBook_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movies:\n",
    "    def __init__(self,book):\n",
    "        self.df = pd.read_pickle(\n",
    "            '/home/colin/Insight_Project/data/smallset_2.pkl')\n",
    "        self.keywords = []\n",
    "        self.webpage = \"\"\n",
    "        self.title = \"\"\n",
    "        self.coverlink = \"\"\n",
    "        self.second_best = \"\"\n",
    "        self.third_best = \"\"\n",
    "        self.combine_frames(book)\n",
    "        self.best_movie()\n",
    "        self.grab_movie_img()\n",
    "\n",
    "    def combine_frames(self,book):\n",
    "        Book_df = book.df\n",
    "        self.df = Book_df.append(self.df).reset_index(drop=True)\n",
    "\n",
    "    def best_movie(self):\n",
    "        # Determine which movie has the most features in common with the book\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        temp_df = self.df.copy()\n",
    "        print('b4 mlb')\n",
    "        clean_df = temp_df.join(pd.DataFrame(mlb.fit_transform(temp_df.pop('Keywords')),\n",
    "                                   columns=mlb.classes_,\n",
    "                                   index=temp_df.index))\n",
    "        print('post mlb')\n",
    "        sparse_df = clean_df.drop(columns='Name')\n",
    "        print('pre NN')\n",
    "        print(sparse_df.shape)\n",
    "        nbrs = NearestNeighbors(n_neighbors=4, algorithm='ball_tree').fit(sparse_df.iloc[1:])\n",
    "        distances, top_three = nbrs.kneighbors(pd.DataFrame(sparse_df.iloc[0]).T)\n",
    "        print('post NN',top_three)\n",
    "        self.keywords = self.df['Keywords'][top_three[0][0]+1]\n",
    "        self.title = clean_df['Name'][top_three[0][0]+1]\n",
    "        self.second_best =clean_df['Name'][top_three[0][1]+1]\n",
    "        self.third_best = clean_df['Name'][top_three[0][2]+1]\n",
    "        #return H, clean_df['Name']\n",
    "        #return sparse_df\n",
    "\n",
    "    def grab_movie_img(self):\n",
    "        query = self.title + ' site:imdb.com'\n",
    "        g_clean = []\n",
    "        self.webpage = 'https://www.google.com/search?client=ubuntu&channel=fs&q={}&ie=utf-8&oe=utf-8'.format(\n",
    "            query)  # this is the actual query we are going to scrape\n",
    "        html = requests.get(self.webpage)\n",
    "        soup = BeautifulSoup(html.text)\n",
    "        a = soup.find_all('a')  # a is a list\n",
    "        a[0].get('href')\n",
    "        for i in a:\n",
    "            k = i.get('href')\n",
    "            try:\n",
    "                m = re.search(\"(?P<url>https?://[^\\s]+)\", k)\n",
    "                n = m.group(0)\n",
    "                rul = n.split('&')[0]\n",
    "                domain = urlparse(rul)\n",
    "                if re.search(\"google.com\", domain.netloc):\n",
    "                    continue\n",
    "                else:\n",
    "                    g_clean.append(rul)\n",
    "            except:\n",
    "                continue\n",
    "        url = g_clean[0]\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.text)\n",
    "        self.coverlink = soup.find(class_='poster').img['src']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Book('oryx and crake',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Book' object has no attribute 'keywords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c39ed4ace8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Book' object has no attribute 'keywords'"
     ]
    }
   ],
   "source": [
    "test.keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
